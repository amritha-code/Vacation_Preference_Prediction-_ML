# -*- coding: utf-8 -*-
"""Construct week ML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1whXbm_iz5dvHqveSTEwPnb56s6Mrvjn0

# ***MACHINE LEARNING CONSTRUCT WEEK PROJECT***
# ***PREDICTING VACATION PREFERENCES : MOUNTAINS vs BEACHES***
"""

# Import required libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report
import warnings
warnings.filterwarnings("ignore")

# Load the dataset
data = pd.read_csv('/content/mountains_vs_beaches_preferences.csv')

#Display the first 5 rows
data.head()

#Shape of the dataset
print(f"Dataset contains {data.shape[0]} rows and {data.shape[1]} columns")

data.info()

"""# ***Data Cleaning and Preprocessing***



"""

#Check for missing values
data.isnull().sum()

# Encode categorical variables
# Label Encoding for binary variables
binary_cols = ['Pets', 'Environmental_Concerns']
for col in binary_cols:
    data[col] = data[col].astype(int)

# Label Encoding for categorical columns with multiple categories
label_enc_cols = ['Gender', 'Education_Level', 'Preferred_Activities', 'Location', 'Favorite_Season']
le = LabelEncoder()
for col in label_enc_cols:
   data[col] = le.fit_transform(data[col])

# Scale numerical features
scaler = StandardScaler()
num_cols = ['Age', 'Income', 'Travel_Frequency', 'Vacation_Budget', 'Proximity_to_Mountains', 'Proximity_to_Beaches']
data[num_cols] = scaler.fit_transform(data[num_cols])

"""# ***Exploratory Data Analysis (EDA)***"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Create the histograms
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.histplot(data['Proximity_to_Mountains'], kde=True)  #kde adds a kernel density estimate
plt.title('Distribution of Proximity to Mountains')
plt.xlabel('Proximity to Mountains')
plt.ylabel('Frequency')


plt.subplot(1, 2, 2)
sns.histplot(data['Proximity_to_Beaches'], kde=True)
plt.title('Distribution of Proximity to Beaches')
plt.xlabel('Proximity to Beaches')
plt.ylabel('Frequency')

plt.tight_layout() # Adjusts subplot params for a tight layout.
plt.show()

# Analyze the influence of proximity to mountains and beaches on preference
# Calculate the mean preference for different proximity levels
mountain_preference = data.groupby('Proximity_to_Mountains')['Preference'].mean()
beach_preference = data.groupby('Proximity_to_Beaches')['Preference'].mean()


# Further analysis could involve:
# - Correlation analysis between proximity and preference
# - Regression analysis to model the relationship

# Plotting mean preferences
plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
plt.plot(mountain_preference.index, mountain_preference.values)
plt.title('Mean Preference vs. Proximity to Mountains')
plt.xlabel('Proximity to Mountains')
plt.ylabel('Mean Preference')

plt.subplot(1, 2, 2)
plt.plot(beach_preference.index, beach_preference.values)
plt.title('Mean Preference vs. Proximity to Beaches')
plt.xlabel('Proximity to Beaches')
plt.ylabel('Mean Preference')


plt.tight_layout()
plt.show()

# Influence of proximity on preference
plt.figure(figsize=(12, 5))
sns.scatterplot(data=data, x="Proximity_to_Mountains", y="Proximity_to_Beaches", hue="Preference", palette="coolwarm")
plt.title("Influence of Proximity on Vacation Preference")
plt.show()

# Distribution of preferences by age, income, and travel frequency
plt.figure(figsize=(16, 5))  # Initialize the figure with a specified size

# Loop through columns and create subplots
for i, col in enumerate(['Age', 'Income', 'Travel_Frequency'], start=1):
    plt.subplot(1, 3, i)  # Define a 1x3 grid for subplots, place the plot in the i-th position
    sns.histplot(data=data, x=col, hue="Preference", kde=True)  # Plot with seaborn's histplot
    plt.title(f'Distribution of {col} by Preference')  # Title for each subplot

# Show all subplots together
plt.tight_layout()  # Optional: Adjust subplot parameters to give padding
plt.show()

# 2.  Relationship of other features and Preference
# Box plots for numerical features against Preference
numerical_features = ['Age', 'Income', 'Travel_Frequency', 'Vacation_Budget', 'Proximity_to_Mountains', 'Proximity_to_Beaches']
for feature in numerical_features:
    plt.figure(figsize=(8, 6))
    sns.boxplot(x='Preference', y=feature, data=data)
    plt.title(f'Distribution of {feature} by Preference')
    plt.show()

# 3.  Pairplot to visualize relationships among multiple features and Preference
# Selected a subset of features for clarity
sns.pairplot(data, vars=['Age', 'Income', 'Travel_Frequency', 'Vacation_Budget', 'Proximity_Distance', 'Preference'], hue='Preference')
plt.suptitle('Pairplot of Selected Features', y=1.02)
plt.show()

# 4. Heatmap for correlation matrix
# Calculate the correlation matrix
correlation_matrix = data.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Matrix of Features')
plt.show()

# Pair plot to show relationships of proximity and other features by preference
sns.pairplot(data, vars=['Proximity_to_Mountains', 'Proximity_to_Beaches'],
             hue="Preference", palette="coolwarm", height=2.5)
plt.suptitle("Pair Plot of Proximity Features by Preference", y=1.02)
plt.show()

# Preferred activities for each type of vacation preference
sns.countplot(data=data, x="Preferred_Activities", hue="Preference")
plt.title("Preferred Activities by Vacation Type")
plt.show()

"""# ***Feature Engineering***"""

# Interaction feature: Difference between Proximity to Mountains and Proximity to Beaches
data['Proximity_Difference'] = data['Proximity_to_Mountains'] - data['Proximity_to_Beaches']

#Distance-based Feature (Euclidean distance)
data['Proximity_Distance'] = ((data['Proximity_to_Mountains'] - data['Proximity_to_Beaches'])**2 + (data['Proximity_to_Mountains'] + data['Proximity_to_Beaches'])**2)**0.5

"""# ***Model Development***"""

# Define target and features
X = data.drop(columns=["Preference"])
y = data["Preference"]

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
"Logistic Regression": LogisticRegression(),
"Decision Tree": DecisionTreeClassifier(),
"Random Forest": RandomForestClassifier()
}

"""# ***Model Evaluation***"""

# Check for overfitting with cross-validation
for model_name, model in models.items():
     cv_scores = cross_val_score(model, X, y, cv=5)
print(f"{model_name} Cross-Validation Accuracy: {cv_scores.mean()}")

# Train and evaluate models
for model_name, model in models.items():
    model.fit(X_train, y_train)  # Ensure each model is fitted
    y_pred = model.predict(X_test)
    print(f"--- {model_name} ---")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print("Precision:", precision_score(y_test, y_pred))
    print("Recall:", recall_score(y_test, y_pred))
    print("F1 Score:", f1_score(y_test, y_pred))
    print("\n")

# Feature Importance for Random Forest
if "Random Forest" in models:
    rf_model = models["Random Forest"]  # Retrieve the fitted Random Forest model
    feature_importance = rf_model.feature_importances_
    feature_names = X.columns
    feature_df = pd.DataFrame({"Feature": feature_names, "Importance": feature_importance})
    feature_df = feature_df.sort_values(by="Importance", ascending=False)

# Plot Feature Importance
plt.figure(figsize=(10, 6))
sns.barplot(data=feature_df, x="Importance", y="Feature")
plt.title("Feature Importance in Random Forest")
plt.show()

